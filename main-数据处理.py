import os
import json
import gradio as gr
import argparse  # æ·»åŠ å‚æ•°è§£æå™¨
import torch
import concurrent.futures
import multiprocessing
import time
import numpy as np
from docagent.retrieval.embedding.gemini_embedding import VLLMQwenEmbedding
from docagent.retrieval.database.milvus_database import ChromaDatabase
from docagent.retrieval.retriever.simple_retriever import SimpleRetriever

# ä¿®æ”¹æ—¶é—´å¤„ç†å‡½æ•°
def process_publish_time(time_str):
    """å¤„ç†ä¸åŒæ ¼å¼çš„å‘å¸ƒæ—¶é—´"""
    if not time_str or time_str == "Not Available":
        return None
    
    try:
        # å¤„ç† ISO æ ¼å¼æ—¶é—´ (å¦‚ "2022-11-21T19:10:33.302000Z")
        if 'T' in time_str:
            return int(time_str.split('T')[0].split('-')[0])
        
        # å¤„ç†ç®€å•æ—¥æœŸæ ¼å¼ (å¦‚ "2025-01-02")
        if '-' in time_str:
            return int(time_str.split('-')[0])
        
        # å¦‚æœå·²ç»æ˜¯æ•´æ•°ï¼Œç›´æ¥è¿”å›
        if isinstance(time_str, (int, float)):
            return int(time_str)
            
        return None
    except Exception as e:
        print(f"âš ï¸ æ—¶é—´æ ¼å¼å¤„ç†é”™è¯¯: {time_str}, é”™è¯¯: {str(e)}")
        return None

# ä¿®æ”¹ä½œè€…å¤„ç†å‡½æ•°
def process_authors(authors):
    """å¤„ç†ä½œè€…åˆ—è¡¨ï¼Œæ¸…ç†æ ¼å¼"""
    if not authors:
        return ""
    
    if isinstance(authors, list):
        # å¤„ç†åˆ—è¡¨ä¸­çš„æ¯ä¸ªä½œè€…
        processed_authors = []
        for author in authors:
            if author and isinstance(author, str):
                # ç§»é™¤å¤šä½™çš„å¤§æ‹¬å·å’Œç©ºæ ¼
                author = author.strip().strip('{}').strip()
                if author:
                    # å¦‚æœä½œè€…å­—ç¬¦ä¸²ä¸­åŒ…å« "and"ï¼Œåˆ™åˆ†å‰²
                    if " and " in author:
                        and_authors = [a.strip() for a in author.split(" and ") if a.strip()]
                        processed_authors.extend(and_authors)
                    else:
                        processed_authors.append(author)
        return ", ".join(processed_authors)
    elif isinstance(authors, str):
        # å¤„ç†å­—ç¬¦ä¸²å½¢å¼çš„ä½œè€…åˆ—è¡¨
        authors = authors.strip()
        if " and " in authors:
            # åˆ†å‰²å¹¶å¤„ç† "and" åˆ†éš”çš„ä½œè€…
            and_authors = [a.strip() for a in authors.split(" and ") if a.strip()]
            return ", ".join(and_authors)
        return authors
    return ""

def process_paper(paper):
    """å¤„ç†å•ç¯‡è®ºæ–‡æ•°æ®"""
    try:
        # å¤„ç†å­—æ®µåç§°æ˜ å°„
        if "abstract" in paper:
            paper["summary"] = paper.pop("abstract")
        if "journal_name" in paper:
            paper["venue"] = paper.pop("journal_name")
        if "publish_time" in paper:
            paper["published"] = paper.pop("publish_time")
        
        # å¤„ç†ä½œè€…åˆ—è¡¨
        if "authors" in paper:
            paper["authors"] = process_authors(paper["authors"])
        
        # å¤„ç†å‘å¸ƒæ—¶é—´
        if "published" in paper:
            processed_time = process_publish_time(paper["published"])
            if processed_time is not None:
                paper["published"] = processed_time
            else:
                return None
        
        # ç¡®ä¿å¿…è¦å­—æ®µå­˜åœ¨
        if not all(key in paper for key in ["title", "authors", "summary"]):
            return None
        
        # å¤„ç†æœŸåˆŠåç§°
        if "venue" not in paper:
            paper["venue"] = "æœªçŸ¥"
        elif not paper["venue"]:
            paper["venue"] = "æœªçŸ¥"
        elif isinstance(paper["venue"], str):
            paper["venue"] = paper["venue"].strip()
            if paper["venue"] == "":
                paper["venue"] = "æœªçŸ¥"
        
        return paper
    except Exception as e:
        print(f"âš ï¸ å¤„ç†è®ºæ–‡æ•°æ®æ—¶å‡ºé”™: {str(e)}")
        return None

# ç³»ç»Ÿåˆå§‹åŒ–å‡½æ•° - å¤šGPUå¹¶è¡Œå¤„ç†
def initialize_system(data_dir="/home/dataset-assist-0/data/paperagent/data", reset_db=False, gpu_count=8, data_parallel_rank=0, data_parallel_size=1):
    """ç³»ç»Ÿåˆå§‹åŒ–å‡½æ•°ï¼Œä»æŒ‡å®šæ–‡ä»¶å¤¹åŠ è½½æ‰€æœ‰JSONæ–‡ä»¶ï¼Œåˆ©ç”¨å¤šGPUå¹¶è¡Œå¤„ç†
    
    Parameters:
    -----------
    data_dir: str
        æ•°æ®ç›®å½•è·¯å¾„
    reset_db: bool
        æ˜¯å¦é‡ç½®æ•°æ®åº“
    gpu_count: int
        å¯ç”¨GPUæ•°é‡
    data_parallel_rank: int
        æ•°æ®å¹¶è¡Œç»„çš„æ’åï¼ˆ0æˆ–1ï¼‰
    data_parallel_size: int
        æ•°æ®å¹¶è¡Œç»„çš„æ•°é‡ï¼ˆé€šå¸¸ä¸º2ï¼‰
    """
    try:
        start_time = time.time()
        
        # åˆå§‹åŒ–åµŒå…¥æ¨¡å‹
        tensor_parallel_size = 4  # ä½¿ç”¨4ä¸ªGPUåšå¼ é‡å¹¶è¡Œ
        print(f"â³ åˆå§‹åŒ–åµŒå…¥æ¨¡å‹ (tensor_parallel_size={tensor_parallel_size})...")
        embedding = VLLMQwenEmbedding(tensor_parallel_size=tensor_parallel_size)
        print("âœ… åµŒå…¥æ¨¡å‹åˆå§‹åŒ–å®Œæˆ")

        # åˆå§‹åŒ–æ•°æ®åº“å’Œæ£€ç´¢å™¨
        collection_name = f"papers0520_dp{data_parallel_rank}" if data_parallel_size > 1 else "papers0520"
        print(f"ğŸ’¾ ä½¿ç”¨é›†åˆ: {collection_name}")
        database = ChromaDatabase(collection_name=collection_name, dim=embedding.embedding_dim)
        
        # å¦‚æœéœ€è¦é‡ç½®æ•°æ®åº“ï¼Œåˆ™åˆ é™¤é›†åˆé‡æ–°åˆ›å»º
        if reset_db:
            try:
                database.client.delete_collection("papers0520")
                database.collection = database.client.create_collection(
                    name="papers0520",
                    metadata={"hnsw:space": "cosine", "dimension": embedding.embedding_dim}
                )
                database.doc_count = 0
                print("âœ… æ•°æ®åº“å·²é‡ç½®")
            except Exception as e:
                print(f"âš ï¸ é‡ç½®æ•°æ®åº“å¤±è´¥: {str(e)}")
        
        retriever = SimpleRetriever(embedding, database)
        print("âœ… æ•°æ®åº“å’Œæ£€ç´¢å™¨åˆå§‹åŒ–å®Œæˆ")
        
        # æ£€æŸ¥æ•°æ®ç›®å½•æ˜¯å¦å­˜åœ¨
        if not os.path.exists(data_dir):
            print(f"âš ï¸ æ•°æ®ç›®å½• {data_dir} ä¸å­˜åœ¨ï¼Œç³»ç»Ÿå°†ä»¥ç©ºæ•°æ®åº“å¯åŠ¨")
            return retriever
        
        # è·å–æ‰€æœ‰JSONæ–‡ä»¶
        json_files = [f for f in os.listdir(data_dir) if f.endswith('.json')]
        json_files.sort()  # æŒ‰æ–‡ä»¶åæ’åº
        total_files = len(json_files)
        
        if total_files == 0:
            print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•JSONæ–‡ä»¶ï¼Œç³»ç»Ÿå°†ä»¥ç©ºæ•°æ®åº“å¯åŠ¨")
            return retriever
            
        # åˆ†å‰²æ–‡ä»¶åˆ—è¡¨å®ç°æ•°æ®å¹¶è¡Œ
        if data_parallel_size > 1:
            files_per_group = total_files // data_parallel_size
            start_idx = data_parallel_rank * files_per_group
            end_idx = start_idx + files_per_group if data_parallel_rank < data_parallel_size - 1 else total_files
            json_files = json_files[start_idx:end_idx]
            print(f"ğŸ“Š æ•°æ®å¹¶è¡Œç»„ {data_parallel_rank+1}/{data_parallel_size}ï¼Œå¤„ç† {len(json_files)}/{total_files} ä¸ªæ–‡ä»¶")
        
        print(f"â³ å¼€å§‹å¤„ç† {len(json_files)} ä¸ªæ–‡ä»¶...")
        total_papers = 0
        file_paths = [os.path.join(data_dir, filename) for filename in json_files]
        processed_files = 0
        
        # åˆ›å»ºè¿›ç¨‹æ± ï¼Œå¤„ç†æ¯ä¸ªæ–‡ä»¶
        with concurrent.futures.ThreadPoolExecutor(max_workers=16) as executor:
            for batch_start in range(0, len(file_paths), 10):  # æ¯æ¬¡å¤„ç†10ä¸ªæ–‡ä»¶
                batch_end = min(batch_start + 10, len(file_paths))
                batch_files = file_paths[batch_start:batch_end]
                
                # å¹¶è¡Œå¤„ç†æ¯ä¸ªæ–‡ä»¶
                file_papers_list = []
                for file_path in batch_files:
                    try:
                        # è¯»å–JSONæ–‡ä»¶
                        with open(file_path, 'r', encoding='utf-8') as f:
                            papers = json.load(f)
                        
                        # å¤„ç†è®ºæ–‡æ•°æ®
                        file_papers = []
                        if isinstance(papers, list):
                            paper_results = list(executor.map(process_paper, papers))
                            file_papers = [p for p in paper_results if p]
                        elif isinstance(papers, dict):
                            processed_paper = process_paper(papers)
                            if processed_paper:
                                file_papers.append(processed_paper)
                        
                        file_papers_list.extend(file_papers)
                    except Exception as e:
                        print(f"âŒ å¤„ç†æ–‡ä»¶å‡ºé”™: {os.path.basename(file_path)}")
                
                processed_files += len(batch_files)
                progress = processed_files / len(file_paths) * 100
                print(f"ğŸ”„ å·²å¤„ç†: {processed_files}/{len(file_paths)} ä¸ªæ–‡ä»¶ ({progress:.1f}%)")
                
                # å¤„ç†å®Œä¸€æ‰¹æ–‡ä»¶åç”ŸæˆåµŒå…¥å¹¶å¯¼å…¥æ•°æ®åº“
                if file_papers_list:
                    # åˆ†æ‰¹å¤„ç†åµŒå…¥ï¼Œæé«˜æ‰¹å¤„ç†å¤§å°ä»¥æå‡GPUåˆ©ç”¨ç‡
                    for i in range(0, len(file_papers_list), 128):
                        batch_papers = file_papers_list[i:i+128]
                        # ä½¿ç”¨æ£€ç´¢å™¨æ·»åŠ æ–‡æ¡£
                        retriever.add_batched_documents(batch_papers, batch_size=128)
                    
                    total_papers += len(file_papers_list)
                    print(f"ğŸ“ æ€»è®¡å·²å¯¼å…¥: {total_papers} ç¯‡è®ºæ–‡")
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        if total_papers > 0:
            print(f"\nâœ… å¤„ç†å®Œæˆ: å¯¼å…¥ {total_papers} ç¯‡è®ºæ–‡ï¼Œç”¨æ—¶ {processing_time:.2f} ç§’ï¼Œå¹³å‡æ¯æ–‡ä»¶ {processing_time/len(json_files):.2f} ç§’")
        else:
            print("\nâš ï¸ æœªæ‰¾åˆ°æœ‰æ•ˆçš„è®ºæ–‡æ•°æ®ï¼Œç³»ç»Ÿå°†ä»¥ç©ºæ•°æ®åº“å¯åŠ¨")
        
        return retriever

    except Exception as e:
        print(f"âŒ ç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {str(e)}")
        import traceback
        traceback.print_exc()
        return None

# ä¿®æ”¹æ„å»ºMilvusè¿‡æ»¤è¡¨è¾¾å¼å‡½æ•°
def build_filters(journal=None, min_year=None, max_year=None, author=None):
    """æ„å»ºMilvusè¿‡æ»¤è¡¨è¾¾å¼"""
    filters = []
    if journal and journal.strip():
        filters.append(f'venue == "{journal}"')
    
    time_filters = []
    if min_year and min_year != "":
        time_filters.append(f"published >= {int(min_year)}")
    if max_year and max_year != "":
        time_filters.append(f"published <= {int(max_year)}")
    if time_filters:
        filters.append("(" + " AND ".join(time_filters) + ")")
        
    # ä¿®æ”¹ä½œè€…åŒ¹é…é€»è¾‘ï¼Œæ”¯æŒæ¨¡ç³ŠåŒ¹é…å•ä¸ªä½œè€…
    if author and author.strip():
        # ç§»é™¤å¤šä½™çš„ç©ºæ ¼å¹¶åˆ†å‰²ä½œè€…å
        author_terms = [name.strip() for name in author.split(',')]
        author_filters = []
        for term in author_terms:
            if term:  # ç¡®ä¿ä¸æ˜¯ç©ºå­—ç¬¦ä¸²
                # ä½¿ç”¨ LIKE è¿›è¡Œæ¨¡ç³ŠåŒ¹é…ï¼Œä¸åŒºåˆ†å¤§å°å†™
                author_filters.append(f'authors LIKE "%{term}%"')
        if author_filters:
            # ä½¿ç”¨ OR è¿æ¥å¤šä¸ªä½œè€…æ¡ä»¶ï¼ŒåŒ¹é…ä»»æ„ä¸€ä¸ªä½œè€…å³å¯
            filters.append("(" + " OR ".join(author_filters) + ")")
            
    return " AND ".join(filters) if filters else ""

# ä¿®æ”¹æ ¸å¿ƒæ£€ç´¢å‡½æ•°
def search_papers(query_title, query_abstract, top_k=5, journal=None, min_year=None, max_year=None, author=None):
    """æ ¸å¿ƒæ£€ç´¢å‡½æ•°"""
    # ä¿®æ”¹è¾“å…¥éªŒè¯é€»è¾‘ï¼Œå…è®¸ä»…é€šè¿‡ä½œè€…æ£€ç´¢
    if not (query_title or query_abstract or author):
        return "<div class='output-container'><div style='text-align:center;color:#666;'>âš ï¸ è¯·è‡³å°‘è¾“å…¥æ ‡é¢˜ã€æ‘˜è¦æˆ–ä½œè€…åç§°</div></div>"
    
    # æ„å»ºæŸ¥è¯¢æ–‡æœ¬
    query_parts = []
    if query_title:
        query_parts.append(f"Title: {query_title}")
    if query_abstract:
        query_parts.append(f"Abstract: {query_abstract}")
    if author:
        query_parts.append(f"Author: {author}")
    
    query_text = "\n".join(query_parts)
    
    filter_expr = build_filters(journal, min_year, max_year, author)
    try:
        results = retriever.retrieve(query_text=query_text, top_k=top_k, filter_expression=filter_expr)
    except Exception as e:
        return f"<div class='output-container'><div style='text-align:center;color:#666;'>âŒ æ£€ç´¢å¤±è´¥: {str(e)}</div></div>"
    
    if not results:
        return "<div class='output-container'><div style='text-align:center;color:#666;'>ğŸ” æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡</div></div>"

    # å¼€å§‹æ„å»º HTML è¾“å‡ºï¼Œç¡®ä¿åŒ…è£¹åœ¨ .output-container ä¸­
    html_output = ["<div class='output-container'>"]
    for idx, paper in enumerate(results, 1):
        summary_snippet = paper['entity']['summary'][:300] + "..." if len(paper['entity']['summary']) > 300 else \
        paper['entity']['summary']
        link = paper['entity'].get('link', 'æ— é“¾æ¥')
        html_output.append(f"""
            <div class="paper-result">
                <h3>åŒ¹é…ç»“æœ #{idx}</h3>
                <p><strong>ğŸ“° æœŸåˆŠ:</strong> {paper['entity'].get('venue', 'æœªçŸ¥')}</p>
                <p><strong>ğŸ“… å¹´ä»½:</strong> {paper['entity'].get('published', 'æœªçŸ¥')}</p>
                <p><strong>ğŸ“– æ ‡é¢˜:</strong> {paper['entity']['title']}</p>
                <p><strong>ğŸ‘¥ ä½œè€…:</strong> {paper['entity']['authors']}</p>
                <p><strong>ğŸ“„ æ‘˜è¦:</strong> {summary_snippet}</p>
                <p><strong>ğŸ”— é“¾æ¥:</strong> <a href="{link}" target="_blank">{link}</a></p>
            </div>
            """)
    html_output.append("</div>")
    return "".join(html_output)

# ä¿®æ”¹ç»Ÿè®¡å‡½æ•°çš„å‚æ•°å®šä¹‰
def analyze_authors_publications(keyword1, keyword2, keyword3, keyword4, keyword5, keyword6, min_year=None, max_year=None):
    """ç»Ÿè®¡ä½œè€…åœ¨ç‰¹å®šé¢†åŸŸçš„è®ºæ–‡å‘è¡¨æ•°é‡"""
    try:
        # å°†æ‰€æœ‰å…³é”®è¯æ”¾å…¥åˆ—è¡¨å¹¶æ¸…ç†
        keywords_list = [keyword1, keyword2, keyword3, keyword4, keyword5, keyword6]
        keywords = [kw.strip() for kw in keywords_list if kw.strip()]
        if not keywords:
            return "<div class='output-container'><div style='text-align:center;color:#666;'>âš ï¸ è¯·è‡³å°‘è¾“å…¥ä¸€ä¸ªå…³é”®è¯</div></div>"
            
        # å­˜å‚¨æ‰€æœ‰æ£€ç´¢åˆ°çš„è®ºæ–‡ï¼Œä½¿ç”¨å­—å…¸å­˜å‚¨å®Œæ•´è®ºæ–‡ä¿¡æ¯
        papers_dict = {}
        
        # æ„å»ºåŸºç¡€è¿‡æ»¤æ¡ä»¶
        filter_expr = build_filters(min_year=min_year, max_year=max_year)
        # æ·»åŠ æ’é™¤ arxiv çš„æ¡ä»¶
        if filter_expr:
            filter_expr += ' AND venue != "nature"'
        else:
            filter_expr = 'venue != "nature"'
            
        # å¯¹æ¯ä¸ªå…³é”®è¯è¿›è¡Œæ£€ç´¢
        for keyword in keywords:
            try:
                results = retriever.retrieve(query_text=keyword, top_k=200, filter_expression=filter_expr)
                # å­˜å‚¨å®Œæ•´çš„è®ºæ–‡ä¿¡æ¯
                for paper in results:
                    title = paper['entity']['title']
                    if title not in papers_dict:  # é¿å…é‡å¤æ·»åŠ 
                        papers_dict[title] = paper['entity']
            except Exception as e:
                print(f"æ£€ç´¢å…³é”®è¯ '{keyword}' æ—¶å‡ºé”™: {str(e)}")
                continue
                
        if not papers_dict:
            return "<div class='output-container'><div style='text-align:center;color:#666;'>ğŸ” æœªæ‰¾åˆ°ç›¸å…³è®ºæ–‡</div></div>"
        
        # ç»Ÿè®¡ä½œè€…å‘è¡¨æ•°é‡
        author_stats = {}
        # ç›´æ¥ä½¿ç”¨å­˜å‚¨çš„è®ºæ–‡ä¿¡æ¯è¿›è¡Œç»Ÿè®¡
        for paper in papers_dict.values():
            authors = paper['authors'].split(", ")
            for author in authors:
                author = author.strip()
                if author:
                    author_stats[author] = author_stats.get(author, 0) + 1
        
        # æ’åºä½œè€…æŒ‰è®ºæ–‡æ•°é‡
        sorted_authors = sorted(author_stats.items(), key=lambda x: x[1], reverse=True)
        
        # æ„å»ºHTMLè¾“å‡º
        html_output = ["<div class='output-container'>"]
        html_output.append(f"<h2>ç ”ç©¶é¢†åŸŸä½œè€…è®ºæ–‡å‘è¡¨ç»Ÿè®¡</h2>")
        html_output.append(f"<p>æ£€ç´¢å…³é”®è¯: {', '.join(keywords)}</p>")
        html_output.append(f"<p>æ€»å…±æ‰¾åˆ°ç›¸å…³è®ºæ–‡: {len(papers_dict)} ç¯‡</p>")
        html_output.append("<div class='stats-container'>")
        
        for idx, (author, count) in enumerate(sorted_authors[:30], 1):  # æ˜¾ç¤ºå‰30å
            html_output.append(f"""
                <div class="author-stat-card">
                    <h3>#{idx} {author}</h3>
                    <p class="paper-count">å‘è¡¨è®ºæ–‡æ•°ï¼š{count}</p>
                </div>
            """)
        
        html_output.append("</div></div>")
        return "".join(html_output)
    
    except Exception as e:
        return f"<div class='output-container'><div style='text-align:center;color:#666;'>âŒ ç»Ÿè®¡å¤±è´¥: {str(e)}</div></div>"

# CSSæ ·å¼ï¼ˆä¿æŒä¸å˜ï¼‰
css = """
/* å…¨å±€æ ·å¼ */
.container {
    max-width: 1200px;
    margin: 0 auto;
}

/* æ ‡é¢˜æ ·å¼ */
h1 {
    text-align: center;
    color: #2c3e50;
    padding: 20px 0;
    margin-bottom: 30px;
    border-bottom: 2px solid #eee;
}

/* è¾“å…¥é¢æ¿æ ·å¼ */
.input-container {
    background: #ffffff;
    border-radius: 10px;
    padding: 20px;
    box-shadow: 0 2px 6px rgba(0,0,0,0.1);
    margin-bottom: 20px;
}

/* è¾“å…¥æ¡†æ ·å¼ */
.gr-textbox, .gr-textarea {
    border: 1px solid #e0e0e0 !important;
    border-radius: 8px !important;
    padding: 10px !important;
    font-size: 14px !important;
    transition: all 0.3s ease;
}

.gr-textbox:focus, .gr-textarea:focus {
    border-color: #2196f3 !important;
    box-shadow: 0 0 0 2px rgba(33,150,243,0.1) !important;
}

/* æŒ‰é’®æ ·å¼ */
.gr-button {
    border-radius: 8px !important;
    padding: 10px 20px !important;
    font-weight: 600 !important;
    transition: all 0.3s ease !important;
}

.gr-button.primary {
    background: #2196f3 !important;
    border: none !important;
}

.gr-button.primary:hover {
    background: #1976d2 !important;
    transform: translateY(-1px);
}

/* ç»“æœé¢æ¿æ ·å¼ */
.output-container {
    background: #ffffff;
    border-radius: 10px;
    padding: 20px !important;
    height: 800px !important;
    overflow-y: auto;
    box-shadow: 0 2px 6px rgba(0,0,0,0.1);
}

.paper-result {
    background: #f8f9fa;
    border-radius: 8px;
    padding: 15px !important;
    margin: 15px 0 !important;
    border: 1px solid #e9ecef;
    transition: all 0.3s ease;
}

.paper-result:hover {
    transform: translateY(-2px);
    box-shadow: 0 4px 8px rgba(0,0,0,0.1);
}

.paper-result h3 {
    color: #1976d2;
    margin: 0 0 15px 0 !important;
    font-size: 18px !important;
}

.paper-result p {
    margin: 8px 0 !important;
    line-height: 1.5;
}

/* åˆ†ç»„æ ·å¼ */
.search-group {
    background: #f8f9fa;
    border-radius: 8px;
    padding: 15px;
    margin-bottom: 15px;
}

.search-group-title {
    font-size: 16px;
    font-weight: 600;
    margin-bottom: 15px;
    color: #2c3e50;
}

/* ç»Ÿè®¡ç»“æœæ ·å¼ */
.stats-container {
    display: grid;
    grid-template-columns: repeat(auto-fill, minmax(250px, 1fr));
    gap: 15px;
    padding: 15px;
}

.author-stat-card {
    background: #ffffff;
    border-radius: 8px;
    padding: 15px;
    box-shadow: 0 2px 4px rgba(0,0,0,0.1);
    transition: all 0.3s ease;
}

.author-stat-card:hover {
    transform: translateY(-2px);
    box-shadow: 0 4px 8px rgba(0,0,0,0.2);
}

.author-stat-card h3 {
    color: #2196f3;
    margin: 0 0 10px 0;
    font-size: 16px;
}

.paper-count {
    font-size: 14px;
    color: #666;
    margin: 0;
}

/* æ ‡ç­¾é¡µæ ·å¼ */
.tab-selected {
    background: #2196f3 !important;
    color: white !important;
}
"""

# ç¤ºä¾‹æœŸåˆŠå’Œå¹´ä»½åˆ—è¡¨ - ç§»åˆ°è¿™é‡Œï¼Œç¡®ä¿åœ¨æ„å»ºç•Œé¢å‰å®šä¹‰
journal_list = ["nature", "science", "cell"]
year_list = [str(year) for year in range(2025, 1899, -1)]

# æ›´æ–°Gradioç•Œé¢
with gr.Blocks(title="AI4så­¦æœ¯è®ºæ–‡æ™ºèƒ½æ£€ç´¢å¹³å°", theme=gr.themes.Soft(), css=css) as interface:
    gr.Markdown("""
        # ğŸ“š AI4så­¦æœ¯è®ºæ–‡æ™ºèƒ½æ£€ç´¢å¹³å°
        ### æ™ºèƒ½æ£€ç´¢æ‚¨éœ€è¦çš„å­¦æœ¯è®ºæ–‡
    """)
    
    # åˆ›å»ºæ ‡ç­¾é¡µ
    with gr.Tabs():
        # è®ºæ–‡æ£€ç´¢æ ‡ç­¾é¡µ
        with gr.Tab("è®ºæ–‡æ£€ç´¢"):
            with gr.Row(equal_height=True):
                # å·¦ä¾§è¾“å…¥é¢æ¿
                with gr.Column(scale=1, min_width=300):
                    with gr.Column(elem_classes="input-container"):
                        # åŸºç¡€æœç´¢åŒºåŸŸ
                        with gr.Column(elem_classes="search-group"):
                            gr.Markdown("### ğŸ“ åŸºç¡€æœç´¢", elem_classes="search-group-title")
                            title_input = gr.Textbox(
                                label="è®ºæ–‡æ ‡é¢˜",
                                placeholder="è¾“å…¥è®ºæ–‡æ ‡é¢˜ï¼ˆé€‰å¡«ï¼‰...",
                            )
                            abstract_input = gr.TextArea(
                                label="è®ºæ–‡æ‘˜è¦",
                                placeholder="è¾“å…¥è®ºæ–‡æ‘˜è¦ï¼ˆé€‰å¡«ï¼‰...",
                                lines=4,
                            )
                        
                        # é«˜çº§ç­›é€‰åŒºåŸŸ
                        with gr.Accordion("ğŸ” é«˜çº§ç­›é€‰", open=True):
                            with gr.Column(elem_classes="search-group"):
                                author_input = gr.Textbox(
                                    label="ä½œè€…å§“å",
                                    placeholder="è¾“å…¥ä½œè€…å§“åï¼Œæ”¯æŒæ¨¡ç³ŠåŒ¹é…...",
                                )
                                journal_input = gr.Dropdown(
                                    label="ç›®æ ‡æœŸåˆŠ",
                                    choices=journal_list,
                                    value=None,
                                )
                                with gr.Row():
                                    min_year_input = gr.Dropdown(
                                        label="èµ·å§‹å¹´ä»½",
                                        choices=year_list,
                                        value=None
                                    )
                                    max_year_input = gr.Dropdown(
                                        label="ç»“æŸå¹´ä»½",
                                        choices=year_list,
                                        value=None
                                    )
                        
                        # æœç´¢æ§åˆ¶åŒºåŸŸ
                        with gr.Row():
                            top_k_input = gr.Slider(
                                minimum=1,
                                maximum=30,
                                value=3,
                                step=1,
                                label="æ˜¾ç¤ºè®ºæ–‡æ•°é‡"
                            )
                            search_btn = gr.Button(
                                "å¼€å§‹æ£€ç´¢",
                                variant="primary",
                                scale=1
                            )

                # å³ä¾§ç»“æœé¢æ¿
                with gr.Column(scale=2):
                    output_panel = gr.HTML(
                        value="<div class='output-container'><div style='text-align:center;color:#666;padding:20px;'>ç­‰å¾…æ£€ç´¢ï¼Œè¯·è¾“å…¥æœç´¢æ¡ä»¶...</div></div>"
                    )

        # ä½œè€…ç»Ÿè®¡æ ‡ç­¾é¡µ
        with gr.Tab("ä½œè€…ç»Ÿè®¡"):
            with gr.Column():
                with gr.Column(elem_classes="input-container"):
                    # ä¿®æ”¹ä¸º6ä¸ªå…³é”®è¯è¾“å…¥æ¡†
                    keywords_inputs = []
                    for i in range(6):
                        keywords_inputs.append(
                            gr.Textbox(
                                label=f"ç ”ç©¶é¢†åŸŸå…³é”®è¯ {i+1}",
                                placeholder=f"è¾“å…¥ç¬¬{i+1}ä¸ªç ”ç©¶é¢†åŸŸå…³é”®è¯...",
                                value="" if i > 0 else "ai for science"
                            )
                        )
                    with gr.Row():
                        stat_min_year = gr.Dropdown(
                            label="èµ·å§‹å¹´ä»½",
                            choices=year_list,
                            value=None
                        )
                        stat_max_year = gr.Dropdown(
                            label="ç»“æŸå¹´ä»½",
                            choices=year_list,
                            value=None
                        )
                    analyze_btn = gr.Button("å¼€å§‹ç»Ÿè®¡", variant="primary")
                
                stats_output = gr.HTML(
                    value="<div class='output-container'><div style='text-align:center;color:#666;'>ç­‰å¾…ç»Ÿè®¡...</div></div>"
                )
    
    # äº‹ä»¶ç»‘å®š
    search_btn.click(
        fn=search_papers,
        inputs=[title_input, abstract_input, top_k_input, journal_input, min_year_input, max_year_input, author_input],
        outputs=output_panel
    )
    
    analyze_btn.click(
        fn=analyze_authors_publications,
        inputs=[*keywords_inputs, stat_min_year, stat_max_year],
        outputs=stats_output
    )

if __name__ == "__main__":
    # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser(description="AI4så­¦æœ¯è®ºæ–‡æ™ºèƒ½æ£€ç´¢å¹³å°")
    parser.add_argument('--reset', action='store_true', help='é‡ç½®æ•°æ®åº“å¹¶é‡æ–°å¯¼å…¥æ‰€æœ‰æ•°æ®')
    parser.add_argument('--port', type=int, default=8081, help='æœåŠ¡ç«¯å£å·')
    parser.add_argument('--no-share', action='store_true', help='ä¸åˆ›å»ºå…¬å…±é“¾æ¥')
    parser.add_argument('--gpu-count', type=int, default=8, help='ä½¿ç”¨çš„GPUæ•°é‡')
    parser.add_argument('--data-dir', type=str, default="/home/dataset-assist-0/data/paperagent/data", help='æ•°æ®ç›®å½•è·¯å¾„')
    parser.add_argument('--dp-rank', type=int, default=0, help='æ•°æ®å¹¶è¡Œç»„ç¼–å·(0æˆ–1)')
    parser.add_argument('--dp-size', type=int, default=1, help='æ•°æ®å¹¶è¡Œç»„æ•°é‡(é€šå¸¸ä¸º2)')
    parser.add_argument('--merge', action='store_true', help='åˆå¹¶æ‰€æœ‰æ•°æ®å¹¶è¡Œé›†åˆåˆ°ä¸»é›†åˆ')
    args = parser.parse_args()
    
    # å¤„ç†æ•°æ®åˆå¹¶è¯·æ±‚
    if args.merge and args.dp_size > 1:
        try:
            import chromadb
            import time
            
            # ç”Ÿæˆå”¯ä¸€çš„åˆå¹¶æ“ä½œIDï¼ˆä½¿ç”¨æ—¶é—´æˆ³ï¼‰
            merge_id = int(time.time())
            
            print("ğŸ”„ å¼€å§‹åˆå¹¶é›†åˆ...")
            print(f"ğŸ†” æœ¬æ¬¡åˆå¹¶æ“ä½œID: {merge_id}")
            client = chromadb.PersistentClient(path="/home/dataset-assist-0/data/chromadb")
            print(f"ğŸ“‚ ä½¿ç”¨æ•°æ®åº“è·¯å¾„: /home/dataset-assist-0/data/chromadb")
            
            # åˆ—å‡ºæ‰€æœ‰ç°æœ‰é›†åˆè¿›è¡Œæ£€æŸ¥
            existing_collections = client.list_collections()
            print(f"ğŸ“‹ ç°æœ‰é›†åˆåˆ—è¡¨:")
            for coll in existing_collections:
                print(f"  - {coll.name} (ID: {coll.id})")
            
            # æ£€æŸ¥å¹¶è·å–ä¸»é›†åˆ - ä¸ä½¿ç”¨è‡ªå®šä¹‰åµŒå…¥å‡½æ•°ï¼Œä¿æŒä¸æºé›†åˆä¸€è‡´
            main_collection_name = "papers0520"
            try:
                main_collection = client.get_collection(name=main_collection_name)
                original_count = main_collection.count()
                print(f"ğŸ“Š ä¸»é›†åˆ {main_collection_name} ä¸­å·²æœ‰ {original_count} æ¡è®°å½•")
            except:
                main_collection = client.create_collection(name=main_collection_name)
                original_count = 0
                print(f"âœ… å·²åˆ›å»ºä¸»é›†åˆ {main_collection_name}")
            
            # è®°å½•æ€»åˆå¹¶æ–‡æ¡£æ•°
            total_merged_docs = 0
            
            # åˆå¹¶æ‰€æœ‰æ•°æ®å¹¶è¡Œé›†åˆçš„æ•°æ®
            for dp_rank in range(args.dp_size):
                src_collection_name = f"papers0520_dp{dp_rank}"
                try:
                    # ä¸æŒ‡å®šåµŒå…¥å‡½æ•°ï¼Œä½¿ç”¨é›†åˆåŸæœ‰çš„åµŒå…¥å‡½æ•°
                    src_collection = client.get_collection(name=src_collection_name)
                    doc_count = src_collection.count()
                    if doc_count == 0:
                        print(f"âš ï¸ é›†åˆ {src_collection_name} ä¸ºç©ºï¼Œè·³è¿‡")
                        continue
                        
                    print(f"ğŸ“¤ ä»é›†åˆ {src_collection_name} å¯¼å‡º {doc_count} æ¡è®°å½•...")
                    
                    # åˆ†æ‰¹è·å–å¹¶å¯¼å…¥æ•°æ®
                    batch_size = 1000
                    for i in range(0, doc_count, batch_size):
                        # è·å–å½“å‰æ‰¹æ¬¡çš„æ–‡æ¡£å’ŒåµŒå…¥
                        results = src_collection.get(
                            limit=batch_size,
                            offset=i,
                            include=["documents", "embeddings", "metadatas"]
                        )
                        
                        if not results["ids"]:
                            continue
                            
                        # ä¿®æ”¹IDï¼Œæ·»åŠ åˆå¹¶IDå’Œæ¥æºå‰ç¼€ï¼Œç¡®ä¿å¤šæ¬¡åˆå¹¶ä¹Ÿä¸ä¼šé‡å¤
                        modified_ids = [f"merge{merge_id}_dp{dp_rank}_{id}" for id in results["ids"]]
                        
                        # æ·»åŠ åˆ°ä¸»é›†åˆ
                        main_collection.add(
                            ids=modified_ids,
                            embeddings=results["embeddings"],
                            documents=results["documents"],
                            metadatas=results["metadatas"]
                        )
                        
                        # æ›´æ–°åˆå¹¶è®¡æ•°
                        batch_count = len(modified_ids)
                        total_merged_docs += batch_count
                        
                        print(f"âœ… å·²å¤„ç† {min(i+batch_size, doc_count)}/{doc_count} æ¡è®°å½•")
                    
                    print(f"âœ… å·²å®Œæˆé›†åˆ {src_collection_name} çš„åˆå¹¶")
                except Exception as e:
                    print(f"âŒ åˆå¹¶é›†åˆ {src_collection_name} å¤±è´¥: {str(e)}")
            
            final_count = main_collection.count()
            print(f"ğŸ“Š åˆå¹¶å‰ä¸»é›†åˆä¸­æœ‰ {original_count} æ¡è®°å½•")
            print(f"ğŸ“Š åˆå¹¶åä¸»é›†åˆ {main_collection_name} ä¸­æœ‰ {final_count} æ¡è®°å½•")
            print(f"ğŸ“ˆ æœ¬æ¬¡åˆå¹¶æ·»åŠ äº† {final_count - original_count} æ¡è®°å½•")
            print(f"ğŸ“ˆ ç†è®ºä¸Šåº”æ·»åŠ  {total_merged_docs} æ¡è®°å½•")
            print("âœ… é›†åˆåˆå¹¶å®Œæˆ")
            
            # åˆ é™¤æºé›†åˆ
            print("ğŸ—‘ï¸ å¼€å§‹åˆ é™¤æºé›†åˆ...")
            for dp_rank in range(args.dp_size):
                src_collection_name = f"papers0520_dp{dp_rank}"
                try:
                    client.delete_collection(src_collection_name)
                    print(f"âœ… å·²åˆ é™¤æºé›†åˆ {src_collection_name}")
                except Exception as e:
                    print(f"âš ï¸ åˆ é™¤é›†åˆ {src_collection_name} å¤±è´¥: {str(e)}")
            
            print("âœ… æ‰€æœ‰æ“ä½œå®Œæˆ")
            exit(0)
        except Exception as e:
            print(f"âŒ åˆå¹¶é›†åˆå¤±è´¥: {str(e)}")
            import traceback
            traceback.print_exc()
            exit(1)
    
    # è®¾ç½®å¯è§GPU - æ ¹æ®æ•°æ®å¹¶è¡Œç»„ç¼–å·è®¾ç½®
    if args.dp_size > 1:
        gpu_per_group = args.gpu_count // args.dp_size
        visible_gpus = list(range(args.dp_rank * gpu_per_group, (args.dp_rank + 1) * gpu_per_group))
        os.environ["CUDA_VISIBLE_DEVICES"] = ",".join(map(str, visible_gpus))
        print(f"ğŸ”„ æ•°æ®å¹¶è¡Œç»„ {args.dp_rank+1}/{args.dp_size}ï¼Œä½¿ç”¨ GPU {visible_gpus}")
    
    # ç³»ç»Ÿåˆå§‹åŒ–
    print(f"ğŸš€ ç³»ç»Ÿå¯åŠ¨ - ç«¯å£: {args.port}")
    retriever = initialize_system(
        data_dir=args.data_dir, 
        reset_db=args.reset, 
        gpu_count=args.gpu_count // args.dp_size if args.dp_size > 1 else args.gpu_count,
        data_parallel_rank=args.dp_rank,
        data_parallel_size=args.dp_size
    )
    
    # å¯åŠ¨ç•Œé¢
    interface.launch(server_port=args.port, share=not args.no_share)